# -*- coding: utf-8 -*-
"""Assignment_ML_DL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/127YLOItjxf_o76iCHLG8ijMXJOcm6CFU
"""

# Keras
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation
from keras.layers.embeddings import Embedding

## Plot
import plotly.offline as py
import plotly.graph_objs as go
py.init_notebook_mode(connected=True)
import matplotlib as plt

# NLTK
import nltk
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer

# Other
import re
import string
from sklearn.manifold import TSNE

import pandas as pd
import numpy as np

df = pd.read_csv('/content/Question - Sheet1.csv')
df.head()

df.describe()

def ratio(x):
  if x=='Sc':
    return 1
  else:
    return 0;

df['label'] = df.Class.apply(lambda x: ratio(x))

"""Labelling"""

df

"""Processing of the Text"""

import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import re

nltk.download('wordnet')
nltk.download('stopwords')

stop_words = set(stopwords.words("english"))
stop_words.add('rt')
stop_words.remove('not')
lemmatizer = WordNetLemmatizer()
giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|' '[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
mention_regex = '@[\w\-]+'

def clean_text(text):
    text = re.sub('"', "", text)
    text = re.sub(mention_regex, ' ',text) #removing all user names
    text = re.sub(giant_url_regex, ' ', text)  #remocing the urls
    text = text.lower()
    text = re.sub("hm+", "", text) #removing variants of hmmm
    text = re.sub("[^a-z]+", " ", text) #removing all numbers, special chars like @,#,? etc
    text = text.split()
    text = [word for word in text if not word in stop_words]
    #text = [d[word] if word in d else word for word in text]  #replacing some slangs
    text = [lemmatizer.lemmatize(token) for token in text]
    text = [lemmatizer.lemmatize(token, "v") for token in text]
    text = " ".join(text)
    return text

df['processed_tweets'] = df.Question.apply(lambda x: clean_text(x))   # df.review.map(clean_text) Also can be used
df.head()

from sklearn.model_selection import train_test_split as tts

df['processed_tweets'] = df['processed_tweets'].astype(str)

x = df['processed_tweets']
y = df['label']

num_words = 8000
embed_dim = 32
tokenizer = Tokenizer(num_words=num_words,oov_token = "<oov>" )
tokenizer.fit_on_texts(x)
word_index=tokenizer.word_index
sequences = tokenizer.texts_to_sequences(x)
length=[]
for i in sequences:
    length.append(len(i))
print(len(length))
print("Mean is: ",np.mean(length))
print("Max is: ",np.max(length))
print("Min is: ",np.min(length))

pad_length = 24
sequences = pad_sequences(sequences, maxlen = pad_length, truncating = 'pre', padding = 'post')
sequences.shape

x_train,x_test,y_train,y_test = tts(sequences,y,test_size = 0.2)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

"""Using LSTM Binnary Classification"""

model_lstm = Sequential()
model_lstm.add(Embedding(20000, 100, input_length=50))
model_lstm.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model_lstm.add(Dense(1, activation='sigmoid'))
model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model_lstm.fit(x_train, y_train, validation_split=0.2, epochs=50)

model_lstm.evaluate(x_test, y_test)

"""CNN+LSTM"""

vocabulary_size = 20000

def create_conv_model():
    model_conv = Sequential()
    model_conv.add(Embedding(vocabulary_size, 100, input_length=50))
    model_conv.add(Dropout(0.2))
    model_conv.add(Conv1D(64, 5, activation='relu'))
    model_conv.add(MaxPooling1D(pool_size=4))
    model_conv.add(LSTM(100))
    model_conv.add(Dense(1, activation='sigmoid'))
    model_conv.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model_conv

model_conv = create_conv_model()
model_conv.fit(x_train, y_train, validation_split=0.2, epochs = 50)

model_conv.evaluate(x_test, y_test)

"""RNN"""

from keras.layers import Dense, Embedding, Dropout , Activation, Flatten, SimpleRNN
import tensorflow as tf
from keras.layers import GlobalMaxPool1D

recall = tf.keras.metrics.Recall()
precision = tf.keras.metrics.Precision()

model = Sequential([Embedding(num_words, embed_dim, input_length = pad_length),
                   SimpleRNN(8, return_sequences = True),
                   GlobalMaxPool1D(),
                   Dense(20,activation = 'relu'),
                   Dropout(0.25),
                   Dense(1,activation = 'sigmoid')])
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

history = model.fit(x = x_train, y = y_train, epochs = 10,validation_split = 0.2)

predictions = model.predict(x_test)
predict = []
for i in predictions:
    predict.append(np.argmax(i))

from sklearn import metrics

print(metrics.classification_report(y_test, predict))

"""Applying Machine Learning Algoritham"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.svm import LinearSVC
from sklearn.svm import SVC

vectorizer = TfidfVectorizer(max_features = 1000 )
# tokenize and build vocab

vectorizer.fit(x)
# summarize

print(len(vectorizer.vocabulary_))
print(vectorizer.idf_.shape)

x_tfidf = vectorizer.transform(x).toarray()
print(x_tfidf.shape)

"""Linear SVM"""

svm_model = LinearSVC(class_weight='balanced',multi_class='crammer_singer',max_iter = -1).fit(x_train, y_train)
svm_model_predict = svm_model.predict(x_test)
svm_report = classification_report(y_test, svm_model_predict )
print(svm_report)

"""Logistic Regression"""

logistic_reg_model = LogisticRegression(n_jobs = -1, penalty='l2', multi_class='multinomial',class_weight = 'balanced',verbose=1).fit(x_train,y_train)
lr_model_predict = logistic_reg_model.predict(x_test)
lr_model_report = classification_report(y_test, lr_model_predict)
print(lr_model_report)

"""Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
dct = DecisionTreeClassifier(criterion='entropy', random_state=1)
decision_tree_model = dct.fit(x_train,y_train)
decision_tree_model_predict = decision_tree_model.predict(x_test)
decision_tree_report = classification_report(y_test,decision_tree_model_predict)
print(decision_tree_report)

"""Random Forest"""

from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=20)
random_forest_model = clf.fit(x_train,y_train)
random_forest_model_predict = random_forest_model.predict(x_test)
random_forest_report = classification_report(y_test,random_forest_model_predict)
print(random_forest_report)

"""Multi Class Naive Bayes"""

from sklearn.naive_bayes import MultinomialNB
model = MultinomialNB()
naive_bayes_model = model.fit(x_train,y_train)
naive_bayes_model_predict = naive_bayes_model.predict(x_test)
naive_bayes_report = classification_report(y_test,naive_bayes_model_predict)
print(naive_bayes_report)

"""Adaboost"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier() 
clf = AdaBoostClassifier(n_estimators=100, base_estimator=dt,learning_rate=1)
# training the model
clf.fit(x_train,y_train)
adaboost_model_predict = clf.predict(x_test)
adaboost_model_report = classification_report(y_test, adaboost_model_predict)
print(adaboost_model_report)

"""XGBoost"""

from xgboost import XGBClassifier
model = XGBClassifier()
xgboost_model = model.fit(x_train, y_train)
xgboost_model_predict = xgboost_model.predict(x_test)
xgboost_model_report = classification_report(y_test,xgboost_model_predict)
print(xgboost_model_report)